{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%run cv.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts_features = pd.read_pickle(output_path + '/train_text_features9')\n",
    "test_texts_features = pd.read_pickle(output_path + '/test_text_features9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts_features['positivity'] = pd.read_pickle(output_path + '/positivity_train2').positivity\n",
    "test_texts_features['positivity'] = pd.read_pickle(output_path + '/positivity_test2').positivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts_features.drop('embedding', 1, inplace=True)\n",
    "test_texts_features.drop('embedding', 1, inplace=True)\n",
    "#train_texts_features['embedding_lgb'] = pd.read_pickle(output_path + '/train_30_2_svd').embeddings\n",
    "#test_texts_features['embedding_lgb'] = pd.read_pickle(output_path + '/test_30_2_svd').embeddings\n",
    "train_texts_features['embedding_keras'] = pd.read_pickle(output_path + '/train_80_1_svd').embeddings\n",
    "test_texts_features['embedding_keras'] = pd.read_pickle(output_path + '/test_80_1_svd').embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts_features['embedding'] = pd.read_pickle(output_path + '/train_100_embs').embeddings\n",
    "test_texts_features['embedding'] = pd.read_pickle(output_path + '/test_100_embs').embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assigned_clusters3_15 = pd.read_pickle(output_path + '/assigned_clusters3_15').assigned_clusters3_15\n",
    "train_texts_features['assigned_clusters'] = assigned_clusters3_15[0:train_texts_features.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_pickle(output_path + '/train_data')\n",
    "test_data = pd.read_pickle(output_path + '/test_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat = pd.concat((train_data, test_data), sort=False)[['metadata_ownerId', 'objectId']] \\\n",
    "    .groupby('metadata_ownerId').agg({'objectId': 'count'})\n",
    "stat.rename(columns={'objectId': 'owner_count'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat3 = pd.concat((train_data, test_data), sort=False)[['objectId', 'instanceId_userId']] \\\n",
    "    .groupby('instanceId_userId').agg({'objectId': 'count'})\n",
    "stat3.rename(columns={'objectId': 'user_count'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stat2 = pd.concat((train_data, test_data), sort=False)[['objectId', 'instanceId_userId']] \\\n",
    "#    .groupby('objectId').agg({'instanceId_userId': 'count'})\n",
    "#stat2.rename(columns={'instanceId_userId': 'object_count'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.join(stat, how='left', on='metadata_ownerId')\n",
    "#train_data = train_data.join(stat2, how='left', on='metadata_ownerId')\n",
    "train_data = train_data.join(stat3, how='left', on='instanceId_userId')\n",
    "train_data = train_data.join(train_texts_features.set_index('objectId'), how='inner', on='objectId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.join(stat, how='left', on='metadata_ownerId')\n",
    "#test_data = test_data.join(stat2, how='left', on='metadata_ownerId')\n",
    "test_data = test_data.join(stat3, how='left', on='instanceId_userId')\n",
    "test_data = test_data.join(test_texts_features.set_index('objectId'), how='inner', on='objectId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "qq = train_data[['instanceId_userId', 'objectId', 'audit_timestamp']].groupby(['instanceId_userId', 'objectId']).count()\n",
    "qq.rename(columns={'audit_timestamp': 'edits_count'}, inplace=True)\n",
    "train_data = train_data.join(qq, how='left', on=['instanceId_userId', 'objectId'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "qq = test_data[['instanceId_userId', 'objectId', 'audit_timestamp']].groupby(['instanceId_userId', 'objectId']).count()\n",
    "qq.rename(columns={'audit_timestamp': 'edits_count'}, inplace=True)\n",
    "test_data = test_data.join(qq, how='left', on=['instanceId_userId', 'objectId'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LgbModel(MyModel):\n",
    "    def get_X(self, data):\n",
    "        X = super().get_X(data)\n",
    "        ###!!!!!!\n",
    "        X.drop([c for c in X.columns if c.startswith('embedding') and not c.startswith('embedding_keras')], 1, inplace=True)\n",
    "        X.drop(['end', 'is_ru'], 1, inplace=True)\n",
    "        return X\n",
    "    def fit(self, data):\n",
    "        #data = data.drop_duplicates(['instanceId_userId', 'objectId'], keep='last')\n",
    "        \n",
    "        #owner_stat = data[['metadata_ownerId', 'label']].groupby('metadata_ownerId').mean()\n",
    "        #owner_stat.rename(columns={'label': 'owner_mean_label'}, inplace=True)\n",
    "        #self.owner_stat = owner_stat\n",
    "        #data = data.join(owner_stat, how='left', on='metadata_ownerId')\n",
    "        '''\n",
    "        grp = data[['instanceId_userId', 'label']].groupby('instanceId_userId')\n",
    "        self.owner_stat = grp.sum() / (grp.count() + 1)\n",
    "        self.owner_stat.rename(columns={'label': 'owner_mean_label'}, inplace=True)\n",
    "\n",
    "        owner_sum_stat = grp.sum()\n",
    "        owner_cnt_stat = grp.count()\n",
    "        owner_sum_stat.rename(columns={'label': 'owner_sum_label'}, inplace=True)\n",
    "        owner_cnt_stat.rename(columns={'label': 'owner_cnt_label'}, inplace=True)\n",
    "\n",
    "        data = data.join(owner_sum_stat, how='left', on='instanceId_userId')\n",
    "        data = data.join(owner_cnt_stat, how='left', on='instanceId_userId')\n",
    "        data['owner_mean_label'] = (data['owner_sum_label'] - data['label']) / data['owner_cnt_label']\n",
    "        data.drop(['owner_sum_label', 'owner_cnt_label'], 1, inplace=True)\n",
    "        '''\n",
    "        y = data['label']\n",
    "        lgb_train = lgb.Dataset(self.get_X(data), y)\n",
    "        categorical_feature=['assigned_clusters']\n",
    "        \n",
    "        #if self.verbose >= 2:\n",
    "        print('Starting train: %s' % datetime.datetime.now())\n",
    "        params = self.params.copy()\n",
    "        num_boost_round = params['num_boost_round']\n",
    "        del params['num_boost_round']\n",
    "        #params['objective'] = 'binary'\n",
    "        params['metric'] = 'auc'\n",
    "        self.model = lgb.train(\n",
    "            params,\n",
    "            lgb_train,\n",
    "            num_boost_round=num_boost_round\n",
    "        )\n",
    "    def predict(self, data):\n",
    "        #data = data.join(self.owner_stat, how='left', on='metadata_ownerId')\n",
    "        #data = data.join(self.owner_stat, how='left', on='instanceId_userId')\n",
    "        proba = self.model.predict(self.get_X(data))\n",
    "        return proba\n",
    "\n",
    "class KerasModel(MyModel):\n",
    "    def get_X(self, data):\n",
    "        X = super().get_X(data)\n",
    "        X.drop([c for c in X.columns if c.startswith('embedding') and not c.startswith(self.params['emb'])], 1, inplace=True)\n",
    "        X.drop(['end', 'is_ru'], 1, inplace=True)\n",
    "        return X\n",
    "    def fit(self, data):\n",
    "        params = self.params\n",
    "        self.scaler = MyScaler()\n",
    "        X = self.get_X(data)\n",
    "        self.scaler.fit_transform(X, inplace=True)\n",
    "        y = data['label'].values\n",
    "        \n",
    "        model = keras.models.Sequential()\n",
    "        self.model = model\n",
    "\n",
    "        model.add(keras.layers.Dense(params['n1'], activation = \"relu\", input_shape=(X.shape[1], )))\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "        model.add(keras.layers.LeakyReLU())\n",
    "\n",
    "        model.add(keras.layers.Dropout(params['dropout'], noise_shape=None, seed=1))\n",
    "        model.add(keras.layers.Dense(params['n2'], activation = \"relu\"))\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "        \n",
    "        if 'n3' in params:\n",
    "            model.add(keras.layers.Dropout(params['dropout'], noise_shape=None, seed=1))\n",
    "            model.add(keras.layers.Dense(params['n3'], activation = \"relu\"))\n",
    "            model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "        model.add(keras.layers.Dense(1, activation = \"sigmoid\"))\n",
    "        model.summary()\n",
    "        model.compile(\n",
    "            optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False),\n",
    "            loss = \"binary_crossentropy\",\n",
    "            metrics = [\"accuracy\"]\n",
    "        )\n",
    "        results = model.fit(\n",
    "            X, y,\n",
    "            epochs=params['epochs'],\n",
    "            batch_size=params.get('batch_size', 1024),\n",
    "            verbose=params['verbose']\n",
    "        )\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.get_X(X)\n",
    "        self.scaler.transform(X, inplace=True)\n",
    "        return self.model.predict(X)[:,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat(x):\n",
    "    n = x.shape[0]\n",
    "    return np.add.reduce(x.embedding.values) / n\n",
    "\n",
    "class MyScaler():\n",
    "    def fit_transform(self, X, inplace=False):\n",
    "        assert inplace\n",
    "\n",
    "        self.columns = []\n",
    "        for c in X.columns:\n",
    "            if c != 'label':\n",
    "                self.columns.append(c)\n",
    "        \n",
    "        self.means = []\n",
    "        self.sds = []\n",
    "        self.mins = []\n",
    "        self.maxs = []\n",
    "        for c in self.columns:\n",
    "            self.means.append(np.mean(X[c]))\n",
    "            self.sds.append(np.std(X[c]))\n",
    "            self.mins.append(np.min(X[c]))\n",
    "            self.maxs.append(np.max(X[c]))\n",
    "        return self.transform(X, inplace=inplace)\n",
    "    \n",
    "    def transform(self, X, inplace=False):\n",
    "        assert inplace\n",
    "        for c, mean, sd, mn, mx in zip(self.columns, self.means, self.sds, self.mins, self.maxs):\n",
    "            X[c] = ((X[c] - mean) / sd).astype(np.float32)\n",
    "            #X[c] = ((X[c].astype(np.float64) - float(mn)) / (float(mx) - float(mn))).astype(np.float32)\n",
    "        \n",
    "def create_features(data):\n",
    "    ones = np.repeat(1, data.shape[0])\n",
    "    res = pd.DataFrame({\n",
    "        'instanceId_userId': data['instanceId_userId'],\n",
    "        'objectId': data['objectId'],\n",
    "        'object_type': data['instanceId_objectType'],\n",
    "        'client_type': data['audit_clientType'],\n",
    "#        'object_type_0': data['instanceId_objectType'] == 0,\n",
    "#        'object_type_1': data['instanceId_objectType'] == 1,\n",
    "#        'object_type_2': data['instanceId_objectType'] == 2,\n",
    "#        'client_type_0': data['audit_clientType'] == 0,\n",
    "#        'client_type_1': data['audit_clientType'] == 1,\n",
    "#        'client_type_2': data['audit_clientType'] == 2,\n",
    "        \n",
    "        #'is_longread': data['len'] > 400,\n",
    "        #'len': data['len'],\n",
    "        #'p_len': data['p_len'],\n",
    "        'len_log': np.log(data['len'] + 1).astype(np.float32),\n",
    "        'p_len_log': np.log(data['p_len'] + 1).astype(np.float32),\n",
    "        #'plen_per_len': data['p_len'] / data['len'],\n",
    "        'q_count': np.log(data['q_count'] + 1).astype(np.float32),\n",
    "        'links_count': np.log(data['links_count'] + 1).astype(np.float32),\n",
    "        'emojis_rate': (data['emojis_count'] / data['len']).astype(np.float32),\n",
    "        'upper_rate': (data['upper_count'] / data['len']).astype(np.float32),\n",
    "        #'upper_count': data['upper_count'],\n",
    "        'ok_videos_count': data['ok_videos_count'].clip(upper=1).astype(np.int8),\n",
    "        'ok_groups_count': data['ok_groups_count'].clip(upper=1).astype(np.int8),\n",
    "        'youtube_count': data['youtube_count'].clip(upper=1).astype(np.int8),\n",
    "        #'is_video': (data['youtube_count'] + data['ok_videos_count']).clip(upper=1),\n",
    "        'is_adv': data['is_adv'].astype(np.int8),\n",
    "        'is_recipe': data['is_recipe'].astype(np.int8),\n",
    "  #      'brackets_balance': data['brackets_balance'],\n",
    "        'brackets_balance_log': np.log(data['brackets_balance'].abs() + 1).astype(np.float32),\n",
    "        'quotes_count': np.log(data['quotes_count'] + 1).astype(np.float32),\n",
    "        'mdots_count': np.log(data['mdots_count'] + 1).astype(np.float32),\n",
    "        'e_count': np.log(data['e_count'] + 1).astype(np.float32),\n",
    "        #'assigned_clusters': data['assigned_clusters'],\n",
    "        'owner_count': data['owner_count'],\n",
    "        'old': data['audit_timestamp'] - data['metadata_createdAt'],\n",
    "        #'wtf': data['audit_timestamp'] <= data['metadata_createdAt'],\n",
    "        #'mean_target': data['mean_target'],\n",
    "        'has_phone': data['has_phone'].astype(np.int8),                                     #sm+\n",
    "        'hashes_count': np.log(data['hashes_count'] + 1).astype(np.float32),#---\n",
    "        'is_ru': data['lang'] == 'ru',#---\n",
    "        ###'is_day': data['is_day'],#---\n",
    "        'has_newline': data['lines_count'].clip(upper=1).astype(np.int8),                   #sm+\n",
    "        #'object_count': data['object_count'],\n",
    "        'user_count': data['user_count'],\n",
    "        'end': data['e_end'] | data['q_end'] | data['d_end'],#---\n",
    "        ####'md_count': data['md_count'].clip(upper=1),#---\n",
    "        #'positivity': data['positivity'].fillna(0.5).astype(np.float32),\n",
    "        'metadata_ownerId': data['metadata_ownerId'],\n",
    "        #'unliked': data['unliked'],\n",
    "        #'audit_timestamp_day': data['audit_timestamp'] % (3600*24),\n",
    "        #'edits_count': data['edits_count'].clip(upper=2).astype(np.int8),\n",
    "        'edits_count': np.log(data['edits_count'] + 1).astype(np.float32),\n",
    "    })\n",
    "    #qq = data[['instanceId_userId', 'embedding']].join(\n",
    "    #        data.groupby('instanceId_userId').apply(feat).reset_index().set_index('instanceId_userId'), \n",
    "    #        on='instanceId_userId')\n",
    "    #print('Cosing:')\n",
    "    #res['emb_dist'] = (qq.iloc[:,1] - qq.iloc[:,2]).apply(np.linalg.norm)\n",
    "    #res['emb_cosine_dist'] = qq.iloc[:,1:3].apply(lambda x: scipy.spatial.distance.cosine(*x), axis=1)\n",
    "    #print('Embs:')\n",
    "    \n",
    "    #for e in ('embedding_lgb', 'embedding_keras', 'embedding'):\n",
    "    for e in ('embedding_keras',):\n",
    "    #for e in ('embedding',):\n",
    "        emb = np.stack(data[e])\n",
    "        for j in range(emb.shape[1]):\n",
    "            res['%s_%d' % (e, j)] = emb[:,j].astype(np.float32)\n",
    "\n",
    "    if 'liked' in data.columns:\n",
    "        res['label'] = data['liked']\n",
    "    for c in ('clicked', 'viewed', 'disliked', 'reshared', 'ignored', 'commented', 'complaint', 'unliked'):\n",
    "        if c in data.columns:\n",
    "            res[c] = data[c]\n",
    "            \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 94.7 ms, sys: 95.7 ms, total: 190 ms\n",
      "Wall time: 352 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ttt = create_features(train_data.head(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 18s, sys: 46.5 s, total: 2min 4s\n",
      "Wall time: 2min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = create_features(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = train[train.instanceId_userId % 4 == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = create_features(train_data[train_data.instanceId_userId % 4 == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=5, random_state=2707, shuffle=True)\n",
      "Prepare data: 2019-03-15 13:36:20.121715\n",
      "Fit: 2019-03-15 13:36:30.404223\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 170)               17850     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 170)               680       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 170)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 170)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 170)               29070     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 170)               680       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 171       \n",
      "=================================================================\n",
      "Total params: 48,451\n",
      "Trainable params: 47,771\n",
      "Non-trainable params: 680\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      " - 55s - loss: 0.3973 - acc: 0.8486\n",
      "Epoch 2/10\n",
      " - 55s - loss: 0.3890 - acc: 0.8508\n",
      "Epoch 3/10\n",
      " - 55s - loss: 0.3874 - acc: 0.8508\n",
      "Epoch 4/10\n",
      " - 55s - loss: 0.3864 - acc: 0.8508\n",
      "Epoch 5/10\n",
      " - 56s - loss: 0.3857 - acc: 0.8508\n",
      "Epoch 6/10\n",
      " - 56s - loss: 0.3850 - acc: 0.8509\n",
      "Epoch 7/10\n",
      " - 56s - loss: 0.3844 - acc: 0.8509\n",
      "Epoch 8/10\n",
      " - 54s - loss: 0.3839 - acc: 0.8509\n",
      "Epoch 9/10\n",
      " - 53s - loss: 0.3835 - acc: 0.8509\n",
      "Epoch 10/10\n",
      " - 55s - loss: 0.3831 - acc: 0.8509\n",
      "Starting train: 2019-03-15 13:46:16.927366\n",
      "Predict: 2019-03-15 13:52:49.123944\n",
      "Auc: 2019-03-15 13:53:23.959719\n",
      " 0 - 1 : 0.6596, mean=0.6596\n",
      "Prepare data: 2019-03-15 13:53:24.208585\n",
      "Fit: 2019-03-15 13:53:35.279593\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 170)               17850     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 170)               680       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 170)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 170)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 170)               29070     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 170)               680       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 171       \n",
      "=================================================================\n",
      "Total params: 48,451\n",
      "Trainable params: 47,771\n",
      "Non-trainable params: 680\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      " - 56s - loss: 0.4000 - acc: 0.8475\n",
      "Epoch 2/10\n",
      " - 55s - loss: 0.3919 - acc: 0.8497\n",
      "Epoch 3/10\n",
      " - 56s - loss: 0.3903 - acc: 0.8497\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-156c83538e55>\u001b[0m in \u001b[0;36mcross_validation\u001b[0;34m(model, data, n_folds, n_iters, seed, verbose, split_by, gv)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Fit: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Predict: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-6ee629d48d24>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0msum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-f97f159fd96b>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'verbose'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         )\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "seed_everything(341)\n",
    "cross_validation(MeanModel([KerasModel({\n",
    "    'epochs': 10,\n",
    "    'verbose': 2,\n",
    "    'n1': 170,\n",
    "    'n2': 170,\n",
    "    'dropout': 0.1,\n",
    "    'emb': 'embedding_keras',\n",
    "}), LgbModel({\n",
    "    'boosting_type': 'gbdt',\n",
    "    'min_data_in_leaf': 25,\n",
    "    'lambda_l2': 0.0,\n",
    "    'num_leaves': 30,\n",
    "    'learning_rate': 0.35,\n",
    "    'feature_fraction': 1,\n",
    "    'bagging_fraction': 1,\n",
    "    'bagging_freq': 5,\n",
    "    'num_boost_round': 1000,\n",
    "    'verbose': 0\n",
    "})], [0.5, 0.5]), sub, n_iters=1, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = create_features(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del test_data\n",
    "del train_data\n",
    "del train_texts_features\n",
    "del test_texts_features\n",
    "del sub\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 170)               17850     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 170)               680       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 170)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 170)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 170)               29070     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 170)               680       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 171       \n",
      "=================================================================\n",
      "Total params: 48,451\n",
      "Trainable params: 47,771\n",
      "Non-trainable params: 680\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/30\n",
      " - 344s - loss: 0.4054 - acc: 0.8442\n",
      "Epoch 2/30\n",
      " - 325s - loss: 0.4011 - acc: 0.8447\n",
      "Epoch 3/30\n",
      " - 327s - loss: 0.4000 - acc: 0.8447\n",
      "Epoch 4/30\n",
      " - 325s - loss: 0.3994 - acc: 0.8447\n",
      "Epoch 5/30\n",
      " - 322s - loss: 0.3989 - acc: 0.8447\n",
      "Epoch 6/30\n",
      " - 322s - loss: 0.3986 - acc: 0.8447\n",
      "Epoch 7/30\n",
      " - 331s - loss: 0.3983 - acc: 0.8447\n",
      "Epoch 8/30\n",
      " - 332s - loss: 0.3981 - acc: 0.8447\n",
      "Epoch 9/30\n",
      " - 329s - loss: 0.3980 - acc: 0.8447\n",
      "Epoch 10/30\n",
      " - 337s - loss: 0.3978 - acc: 0.8447\n",
      "Epoch 11/30\n",
      " - 317s - loss: 0.3976 - acc: 0.8447\n",
      "Epoch 12/30\n",
      " - 324s - loss: 0.3975 - acc: 0.8447\n",
      "Epoch 13/30\n",
      " - 323s - loss: 0.3974 - acc: 0.8447\n",
      "Epoch 14/30\n",
      " - 325s - loss: 0.3973 - acc: 0.8447\n",
      "Epoch 15/30\n",
      " - 329s - loss: 0.3972 - acc: 0.8447\n",
      "Epoch 16/30\n",
      " - 316s - loss: 0.3971 - acc: 0.8447\n",
      "Epoch 17/30\n",
      " - 317s - loss: 0.3971 - acc: 0.8447\n",
      "Epoch 18/30\n",
      " - 316s - loss: 0.3970 - acc: 0.8447\n",
      "Epoch 19/30\n",
      " - 318s - loss: 0.3970 - acc: 0.8447\n",
      "Epoch 20/30\n",
      " - 318s - loss: 0.3969 - acc: 0.8447\n",
      "Epoch 21/30\n",
      " - 320s - loss: 0.3968 - acc: 0.8447\n",
      "Epoch 22/30\n",
      " - 318s - loss: 0.3968 - acc: 0.8447\n",
      "Epoch 23/30\n",
      " - 317s - loss: 0.3967 - acc: 0.8447\n",
      "Epoch 24/30\n",
      " - 315s - loss: 0.3967 - acc: 0.8447\n",
      "Epoch 25/30\n",
      " - 317s - loss: 0.3966 - acc: 0.8447\n",
      "Epoch 26/30\n",
      " - 314s - loss: 0.3966 - acc: 0.8447\n",
      "Epoch 27/30\n",
      " - 317s - loss: 0.3965 - acc: 0.8447\n",
      "Epoch 28/30\n",
      " - 315s - loss: 0.3965 - acc: 0.8447\n",
      "Epoch 29/30\n",
      " - 323s - loss: 0.3964 - acc: 0.8447\n",
      "Epoch 30/30\n",
      " - 395s - loss: 0.3964 - acc: 0.8448\n",
      "Starting train: 2019-03-15 16:52:35.437321\n",
      "CPU times: user 10h 19min 52s, sys: 1h 16min 7s, total: 11h 36min\n",
      "Wall time: 3h 32min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "seed_everything(341)\n",
    "model3 = MeanModel([KerasModel({\n",
    "    'epochs': 30,\n",
    "    'verbose': 2,\n",
    "    'n1': 170,\n",
    "    'n2': 170,\n",
    "    'dropout': 0.1,\n",
    "    'emb': 'embedding_keras',\n",
    "}), LgbModel({\n",
    "    'boosting_type': 'gbdt',\n",
    "    'min_data_in_leaf': 25,\n",
    "    'lambda_l2': 0.0,\n",
    "    'num_leaves': 30,\n",
    "    'learning_rate': 0.35,\n",
    "    'feature_fraction': 1,\n",
    "    'bagging_fraction': 1,\n",
    "    'bagging_freq': 5,\n",
    "    'num_boost_round': 1000,\n",
    "    'verbose': 0\n",
    "})], [0.5, 0.5])\n",
    "\n",
    "model3.fit(train)\n",
    "pred3 = model3.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.coefs = [0.5, 0.5]\n",
    "pred3 = model3.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "predT = pd.read_pickle(output_path + '/pred').T1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "submitA3 = predict_to_submit(test, pred3 + predT * 0.2)\n",
    "submitA3.to_csv(output_path + \"/textSubmitA3.csv.gz\", header=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "seed_everything(341)\n",
    "\n",
    "modelT = KerasModel({\n",
    "    'epochs': 30,\n",
    "    'verbose': 2,\n",
    "    'n1': 170,\n",
    "    'n2': 170,\n",
    "    'n3': 85,\n",
    "    'dropout': 0.15,\n",
    "    'emb': 'embedding',\n",
    "})\n",
    "modelT.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12653881, 0.11429563, 0.17078048, ..., 0.05660952, 0.1017186 ,\n",
       "       0.16263291], dtype=float32)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "submitA1 = predict_to_submit(test, pred2 + predT * 0.2)\n",
    "submitA1.to_csv(output_path + \"/textSubmitA2.csv.gz\", header=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'T1': predT, 'pred2': pred2, 'pred': pred}).to_pickle(output_path + '/pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 170)               18190     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 170)               680       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 170)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 170)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 170)               29070     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 170)               680       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 171       \n",
      "=================================================================\n",
      "Total params: 48,791\n",
      "Trainable params: 48,111\n",
      "Non-trainable params: 680\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/30\n",
      " - 353s - loss: 0.4056 - acc: 0.8442\n",
      "Epoch 2/30\n",
      " - 334s - loss: 0.4015 - acc: 0.8447\n",
      "Epoch 3/30\n",
      " - 321s - loss: 0.4001 - acc: 0.8447\n",
      "Epoch 4/30\n",
      " - 330s - loss: 0.3993 - acc: 0.8447\n",
      "Epoch 5/30\n",
      " - 338s - loss: 0.3988 - acc: 0.8447\n",
      "Epoch 6/30\n",
      " - 322s - loss: 0.3984 - acc: 0.8447\n",
      "Epoch 7/30\n",
      " - 316s - loss: 0.3982 - acc: 0.8447\n",
      "Epoch 8/30\n",
      " - 325s - loss: 0.3980 - acc: 0.8447\n",
      "Epoch 9/30\n",
      " - 317s - loss: 0.3978 - acc: 0.8447\n",
      "Epoch 10/30\n",
      " - 324s - loss: 0.3976 - acc: 0.8447\n",
      "Epoch 11/30\n",
      " - 361s - loss: 0.3974 - acc: 0.8447\n",
      "Epoch 12/30\n",
      " - 473s - loss: 0.3973 - acc: 0.8447\n",
      "Epoch 13/30\n",
      " - 320s - loss: 0.3972 - acc: 0.8447\n",
      "Epoch 14/30\n",
      " - 329s - loss: 0.3971 - acc: 0.8447\n",
      "Epoch 15/30\n",
      " - 327s - loss: 0.3970 - acc: 0.8447\n",
      "Epoch 16/30\n",
      " - 382s - loss: 0.3969 - acc: 0.8447\n",
      "Epoch 17/30\n",
      " - 334s - loss: 0.3969 - acc: 0.8447\n",
      "Epoch 18/30\n",
      " - 342s - loss: 0.3968 - acc: 0.8448\n",
      "Epoch 19/30\n",
      " - 350s - loss: 0.3967 - acc: 0.8447\n",
      "Epoch 20/30\n",
      " - 344s - loss: 0.3967 - acc: 0.8448\n",
      "Epoch 21/30\n",
      " - 367s - loss: 0.3966 - acc: 0.8447\n",
      "Epoch 22/30\n",
      " - 399s - loss: 0.3966 - acc: 0.8447\n",
      "Epoch 23/30\n",
      " - 480s - loss: 0.3965 - acc: 0.8448\n",
      "Epoch 24/30\n",
      " - 408s - loss: 0.3965 - acc: 0.8448\n",
      "Epoch 25/30\n",
      " - 328s - loss: 0.3964 - acc: 0.8448\n",
      "Epoch 26/30\n",
      " - 392s - loss: 0.3964 - acc: 0.8448\n",
      "Epoch 27/30\n",
      " - 398s - loss: 0.3963 - acc: 0.8448\n",
      "Epoch 28/30\n",
      " - 349s - loss: 0.3963 - acc: 0.8448\n",
      "Epoch 29/30\n",
      " - 339s - loss: 0.3962 - acc: 0.8448\n",
      "Epoch 30/30\n",
      " - 316s - loss: 0.3962 - acc: 0.8448\n",
      "Starting train: 2019-03-14 15:35:17.684911\n",
      "CPU times: user 9h 53min 35s, sys: 1h 21min 17s, total: 11h 14min 52s\n",
      "Wall time: 4h 8min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "seed_everything(341)\n",
    "'''\n",
    "KerasModel({\n",
    "    'epochs': 30,\n",
    "    'verbose': 2,\n",
    "    'n1': 170,\n",
    "    'n2': 170,\n",
    "    'n3': 85,\n",
    "    'dropout': 0.15,\n",
    "    'emb': 'embedding',\n",
    "}), \n",
    "'''\n",
    "model = MeanModel([KerasModel({\n",
    "    'epochs': 30,\n",
    "    'verbose': 2,\n",
    "    'n1': 170,\n",
    "    'n2': 170,\n",
    "    'dropout': 0.1,\n",
    "    'emb': 'embedding_keras',\n",
    "}), LgbModel({\n",
    "    'boosting_type': 'gbdt',\n",
    "    'min_data_in_leaf': 25,\n",
    "    'lambda_l2': 0.0,\n",
    "    'num_leaves': 30,\n",
    "    'learning_rate': 0.35,\n",
    "    'feature_fraction': 1,\n",
    "    'bagging_fraction': 1,\n",
    "    'bagging_freq': 5,\n",
    "    'num_boost_round': 1000,\n",
    "    'verbose': 0\n",
    "})], [0.5, 0.5])\n",
    "\n",
    "model.fit(train)\n",
    "pred = model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = predict_to_submit(test, pred)\n",
    "submit.to_csv(output_path + \"/textSubmit1.csv.gz\", header=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>len</th>\n",
       "      <th>count</th>\n",
       "      <th>relative_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>66422</td>\n",
       "      <td>0.318806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>43664</td>\n",
       "      <td>0.209574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>27742</td>\n",
       "      <td>0.133154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>6431</td>\n",
       "      <td>0.030867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>913</td>\n",
       "      <td>0.004382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   len  count  relative_count\n",
       "0    2  66422        0.318806\n",
       "1    3  43664        0.209574\n",
       "2    4  27742        0.133154\n",
       "3    8   6431        0.030867\n",
       "4   16    913        0.004382"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens_stat = defaultdict(int)\n",
    "for r in submit:\n",
    "    lens_stat[len(r)] += 1\n",
    "lens_stat = pd.DataFrame([(k, v) for k, v in lens_stat.items()], columns=['len', 'count'])\n",
    "lens_stat['relative_count'] = lens_stat['count'] / lens_stat['count'].sum()\n",
    "lens_stat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang_index</th>\n",
       "      <th>lang</th>\n",
       "      <th>instanceId_userId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>uz</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>sr</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>it</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>fr</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>et</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>hy</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11</td>\n",
       "      <td>ro</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>ky</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>15</td>\n",
       "      <td>uk</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>ms</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>en</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>9</td>\n",
       "      <td>mk</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>be</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>tg</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>bg</td>\n",
       "      <td>1613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>12783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>12</td>\n",
       "      <td>ru</td>\n",
       "      <td>184691</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    lang_index     lang  instanceId_userId\n",
       "0           16       uz                  1\n",
       "1           13       sr                  2\n",
       "2            7       it                  2\n",
       "3            5       fr                 14\n",
       "4            4       et                 15\n",
       "5            6       hy                 15\n",
       "6           11       ro                 24\n",
       "7            8       ky                 35\n",
       "8           15       uk                 43\n",
       "9           10       ms                 45\n",
       "10           3       en                 45\n",
       "11           9       mk                 50\n",
       "12           1       be                259\n",
       "13          14       tg                363\n",
       "14           2       bg               1613\n",
       "15           0  Unknown              12783\n",
       "16          12       ru             184691"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_stat = pd.concat((train_data, test_data), sort=False)[['lang', 'instanceId_userId']].head(200000) \\\n",
    "    .groupby('lang').count().reset_index().sort_values(['instanceId_userId']).reset_index()\n",
    "lang_stat.rename(columns={'index': 'lang_index'}, inplace=True)\n",
    "lang_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''audit_timestamp = train_data.audit_timestamp.apply(lambda x: int(x) // 1000)\n",
    "plt.figure(figsize=(35,15))\n",
    "plt.hist(audit_timestamp, 1800, color='red', alpha=0.5, stacked=True)\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''plt.figure(figsize=(35,15))\n",
    "plt.hist(audit_timestamp[(audit_timestamp % (3600*24) > 7*3600) & (audit_timestamp % (3600*24) < 20*3600)], 1800, color='red', alpha=0.5, stacked=True)\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_day(ts):\n",
    "    ts = int(ts) // 1000\n",
    "    x = ts % 3600*24\n",
    "    return x > 7*3600 and x < 20*3600\n",
    "\n",
    "train_data['is_day'] = train_data.metadata_createdAt.apply(is_day)\n",
    "test_data['is_day'] = test_data.metadata_createdAt.apply(is_day)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
